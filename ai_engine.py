import os
import requests
import urllib3
import json
from parsing.universal_parser import UniversalParser, CONFIGURATIONS as PARSER_CONFIGS

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) #–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –Ω–∞ –≥–æ—Å —Å–∞–π—Ç–∞—Ö

from langchain_core.messages import AIMessage, HumanMessage
from langchain_gigachat.chat_models import GigaChat
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools import tool

GIGACHAT_CREDENTIALS = "MDE5YWJiZTMtNjFhMi03YjQ2LWE0ZWYtZGZhMmQzYjg0OGUyOmU3OTIwZmE5LWY2MjUtNGExMy1hYmNkLWI1Y2NkNzc4N2M2NQ=="

BASE_URL = "https://yazzh.gate.petersburg.ru"

API_CATALOG = {
    # –ñ–∏–≤–æ—Ç–Ω—ã–µ
    "pets": [
        f"{BASE_URL}/mypets/all-category/",
        f"{BASE_URL}/mypets/posts/",
        f"{BASE_URL}/mypets/recommendations/"
        f"{BASE_URL}/mypets/animal-breeds/",
    ],
    # –î–æ–∫—É–º–µ–Ω—Ç—ã –∏ –ú–§–¶
    "documents": [
        f"{BASE_URL}/mfc/all/"
    ],
    # –ó–¥–æ—Ä–æ–≤—å–µ
    "health": [
        f"{BASE_URL}/polyclinics/"
    ],
    # –†–æ–¥–∏—Ç–µ–ª—å
    "iparent": [
        f"{BASE_URL}/iparent/places/categoria/",
        f"{BASE_URL}/dou/", 
        f"{BASE_URL}/dou/available-spots/",
        f"{BASE_URL}/dou/commissions/",
         f"{BASE_URL}/school/stat/",
        f"{BASE_URL}/school/commissions/",
        f"{BASE_URL}/school/helpful/",
        f"{BASE_URL}/school/map/"
    ],
    "social" :[
        f"{BASE_URL}/uk-falsification/",
        f"{BASE_URL}/districts-info/district/",
        f"{BASE_URL}/disconnections/",
        f"{BASE_URL}/gati/orders/work-type-all/"
    ]
}

TOXIC_WORDS = [
    "—Ö—É–π", "—Ö—É–µ", "—Ö—É—ë", "—Ö—É—è",  
    "–ø–∏–∑–¥", "–µ–±–∞", "–µ–±–ª", "–µ–±—Ç", 
    "–±–ª—è–¥", "–±–ª—è—Ç", "–±–ª—è—Ü",    
    "–º—É–¥–∞", "–º–∞–Ω–¥–∞", "–≥–∞–Ω–¥–æ–Ω",   
    "—Å—É–∫", "—Å—É—á–∫",             
    "—Ö–µ—Ä", "—Ö—Ä–µ–Ω", 
    "—Ç—É–ø–æ–π", "—Ç—É–ø–∏—Ü", "—Ç—É–ø–æ—Ä—ã–ª",
    "–∏–¥–∏–æ—Ç", "–¥–µ–±–∏–ª", "–∫—Ä–µ—Ç–∏–Ω", "–∏–º–±–µ—Ü–∏–ª", "–¥–∞—É–Ω",
    "—É—Ä–æ–¥", "—É–±–ª—é–¥–æ–∫", "—Ç–≤–∞—Ä—å", "–º—Ä–∞–∑—å", "—Å–∫–æ—Ç–∏–Ω–∞",
    "–ª–æ—Ö", "–ª–æ—à", "—á–º–æ", "—á–º—ã—Ä",
    "–≥–æ–≤–Ω–æ", "–≥–æ–≤–Ω", "–¥–µ—Ä—å–º", "–∂–æ–ø",
    "–¥—É—Ä–∞–∫", "–¥—É—Ä–∞",
    "–∑–∞—Ç–∫–Ω–∏—Å—å", "—Å–æ—Å–∏", "—É–±—å—é", "—Å–¥–æ—Ö–Ω–∏", "–ø–æ—à–µ–ª –Ω–∞",
    "–Ω–µ–Ω–∞–≤–∏–∂—É", "–±–µ—Å–∏—à—å",
    "–ì–æ–≤–Ω–æ", "–∑–∞–ª—É–ø–∞", "–ø–µ–Ω–∏—Å", "—Ö–µ—Ä", "–¥–∞–≤–∞–ª–∫–∞", "—Ö—É–π", "–±–ª—è–¥–∏–Ω–∞"
    "–ì–æ–ª–æ–≤–∫–∞", "—à–ª—é—Ö–∞", "–∂–æ–ø–∞", "—á–ª–µ–Ω", "–µ–±–ª–∞–Ω", "–ø–µ—Ç—É—Ö", "–ú—É–¥–∏–ª–∞"
    "–†—É–∫–æ–±–ª—É–¥", "—Å—Å–∞–Ω–∏–Ω–∞", "–æ—á–∫–æ", "–±–ª—è–¥—É–Ω", "–≤–∞–≥–∏–Ω–∞"
    "–°—É–∫–∞", "–µ–±–ª–∞–Ω–∏—â–µ", "–≤–ª–∞–≥–∞–ª–∏—â–µ", "–ø–µ—Ä–¥—É–Ω", "–¥—Ä–æ—á–∏–ª–∞"
    "–ü–∏–¥–æ—Ä", "–ø–∏–∑–¥–∞", "—Ç—É–∑", "–º–∞–ª–∞—Ñ—å—è"
    "–ì–æ–º–∏–∫", "–º—É–¥–∏–ª–∞", "–ø–∏–ª–æ—Ç–∫–∞", "–º–∞–Ω–¥–∞"
    "–ê–Ω—É—Å", "–≤–∞–≥–∏–Ω–∞", "–ø—É—Ç–∞–Ω–∞", "–ø–µ–¥—Ä–∏–ª–∞"
    "–®–∞–ª–∞–≤–∞", "—Ö—É–∏–ª–æ", "–º–æ—à–æ–Ω–∫–∞", "–µ–ª–¥–∞"]

def check_toxicity(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–∞—Ç."""
    for word in TOXIC_WORDS:
        if word in text.lower():
            return True
    return False


def extract_data_safe(json_response):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ª—é–±–æ–≥–æ JSON."""
    if isinstance(json_response, list):
        return json_response
    if isinstance(json_response, dict):
        return json_response.get("data") or json_response.get("results") or []
    return []


def ask_agent(user_input: str, chat_history: list = None, extra_context: str = ""):
    """Helper to invoke the agent executor with optional chat history and extra context.

    - user_input: the raw text from the user
    - chat_history: list of LangChain HumanMessage/AIMessage objects
    - extra_context: string with additional context (search or parsing results) appended to input
    """
    if chat_history is None:
        chat_history = []

    # Append context to the human prompt to give the agent additional facts to use
    prompt_input = user_input
    if extra_context:
        prompt_input = f"{user_input}\n\n[CONTEXT]:\n{extra_context}"

    try:
        response = agent_executor.invoke({
            "input": prompt_input,
            "chat_history": chat_history
        })
        # If the agent returns a dictionary or dict-like output, try grabbing 'output' key
        if isinstance(response, dict):
            return response.get('output') or response.get('result') or str(response)
        return str(response)
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ AI: {e}"


def detect_category(text: str) -> list:
    """–ü—Ä–æ—Å—Ç–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–∏–∑ API_CATALOG) –≤ –ø–æ—Ä—è–¥–∫–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.
    """
    text_lower = text.lower()
    scores = {k: 0 for k in API_CATALOG.keys()}

    # Keywords mapping
    keyword_map = {
        'documents': ['–º—Ñ—Ü', '–ø–∞—Å–ø–æ—Ä—Ç', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü', '—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤', '–¥–æ–∫—É–º–µ–Ω—Ç', '–∑–∞–≥—Ä–∞–Ω', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è', '–ø—Ä–æ–ø–∏—Å', '–ø—Ä–æ–ø–∏—Å–∫–∞', '—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ'],
        'pets': ['–≤–∞–∫—Ü–∏–Ω–∞—Ü–∏—è', '–≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä', '–ø–∏—Ç–æ–º–µ—Ü', '–∂–∏–≤–æ—Ç–Ω', '—Å–æ–±–∞–∫', '–∫–æ—à–∫–∞', '–∫—Ä–æ–ª–∏–∫', '–≤–µ—Ç–∫–ª–∏–Ω–∏–∫', '–ø–∏—Ç–æ–º–µ—Ü', '–ø–∏—Ç–æ–º—Ü—ã'],
        'health': ['–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫', '–≤—Ä–∞—á', '–∑–∞–ø–∏—Å—å –∫ –≤—Ä–∞—á—É', '–≤—ã–∑–æ–≤ –≤—Ä–∞—á–∞', '–º–µ–¥–∏—Ü–∏–Ω', '–∑–¥–æ—Ä–æ–≤—å–µ', '–≤–∞–∫—Ü–∏–Ω–∞—Ü–∏—è', '–ø—Ä–∏–≤–∏–≤–∫–∞'],
        'iparent': ['–¥–µ—Ç', '–¥–µ—Ç—Å–∫–∏–π', '—Å–∞–¥', '—à–∫–æ–ª', '–∫–ª–∞—Å—Å', '–º–ª–∞–¥—à', '–¥–µ—Ç—Å', '–ø—Ä–∏—ë–º –≤ —Å–∞–¥', '—É—á–µ–±–Ω'],
        'social': ['—Å–æ—Ü', '–ª—å–≥–æ—Ç', '–ø–æ—Å–æ–±', '—Å—É–±—Å–∏–¥', '–ø–µ–Ω—Å–∏', '–ø–µ–Ω—Å–∏—è', '–ø–æ–¥–¥–µ—Ä–∂–∫', '–∂–∫—Ö', '–æ—Ç–∫–ª—é—á–µ–Ω', '–æ—Ç–∫–ª—é—á–µ–Ω–∏—è']
    }

    # Score keywords
    for cat, keywords in keyword_map.items():
        for kw in keywords:
            if kw in text_lower:
                scores[cat] += 2

    # Score by presence of category names in the text
    for cat in API_CATALOG.keys():
        if cat in text_lower:
            scores[cat] += 3

    # Add small scoring for presence of generic tokens
    generic_map = {
        'documents': ['–¥–æ–∫—É–º–µ–Ω—Ç—ã', '–º—Ñ—Ü', '–ø–∞—Å–ø–æ—Ä—Ç'],
        'pets': ['–ø–∏—Ç–æ–º–µ—Ü', '–≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä', '–∑–æ–æ–ø–∞—Ä–∫'],
    }
    for cat, tokens in generic_map.items():
        for t in tokens:
            if t in text_lower:
                scores[cat] += 1

    # Sort categories by descending score and filter score>0
    ranked = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)
    ranked = [k for k, v in ranked if v > 0]

    # If nothing matched, return all categories as fallback
    if not ranked:
        return list(API_CATALOG.keys())
    return ranked

def search_city_services(query: str, category: str) -> str:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≥–æ—Ä–æ–¥—Å–∫–∏–º —Å–µ—Ä–≤–∏—Å–∞–º.
    Args:
        query: –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä: '–ø–∞—Å–ø–æ—Ä—Ç', '–ø—Ä–∏–≤–∏–≤–∫–∞', '–ø–ª–æ—â–∞–¥–∫–∞', '–º—Ñ—Ü').
        category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è ('pets', 'documents', 'health', 'social', 'iparent').
    """
    print(f"\n[TOOL LOG] üîç –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category.upper()} | –ó–∞–ø—Ä–æ—Å: '{query}'")
    
    urls = API_CATALOG.get(category)
    if not urls:
        return f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è '{category}' –ø–æ–∫–∞ –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞."

    results = []
    headers = {'Accept': 'application/json'}
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–∏—Å–∫–∞ (–≥—Ä—É–±—ã–π —Å—Ç–µ–º–º–∏–Ω–≥)
    query_lower = query.lower()
    search_root = query_lower[:-1] if len(query_lower) > 4 else query_lower

    # –ü—Ä–æ—Ö–æ–¥ –ø–æ –≤—Å–µ–º URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    for url in urls:
        try:
            response = requests.get(url, headers=headers, verify=False, timeout=5)
            if response.status_code == 200:
                data = extract_data_safe(response.json())
                for item in data:
                    # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –æ–±—ä–µ–∫—Ç–∞ –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
                    full_text = (
                        f"{item.get('name', '')} {item.get('title', '')} "
                        f"{item.get('address', '')} {item.get('description', '')} "
                        f"{item.get('text', '')}"
                    ).lower()
                    
                    # –ò—â–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
                    if search_root in full_text:
                        title = item.get('name') or item.get('title') or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                        address = item.get('address')
                        
                        card = f"- {title}"
                        if address:
                            card += f" (üìç {address})"
                        results.append(card)
        except Exception as e:
            print(f"[TOOL ERROR] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {url}: {e}")

    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏
    if not results:
        return f"–ü–æ –∑–∞–ø—Ä–æ—Å—É '{query}' –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}' –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
    
    # –õ–æ–≥–∏–∫–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤—ã–¥–∞—á–∏
    total_found = len(results)
    limit = 10
    shown_results = results[:limit]
    
    output_text = f"""
[–°–ò–°–¢–ï–ú–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –î–õ–Ø AI]:
1. –í—Å–µ–≥–æ –≤ –±–∞–∑–µ –Ω–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {total_found}.
2. –¢—ã –≤–∏–¥–∏—à—å —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ {limit}.
3. –ï–°–õ–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–¨ –°–ü–†–û–°–ò–¢ "–ü–û–ß–ï–ú–£ –¢–ê–ö –ú–ê–õ–û" –ò–õ–ò "–ü–û–ö–ê–ñ–ò –í–°–ï" ‚Äî –ù–ï –ò–©–ò –ó–ê–ù–û–í–û!
   –ü—Ä–æ—Å—Ç–æ –æ–±—ä—è—Å–Ω–∏, —á—Ç–æ –∑–∞–ø–∏—Å–µ–π {total_found}, –Ω–æ —Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ—à—å —Ç–æ–ø-{limit} –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞.
   –ü–æ–ø—Ä–æ—Å–∏ —É—Ç–æ—á–Ω–∏—Ç—å —Ä–∞–π–æ–Ω.

[–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û–ò–°–ö–ê]:
""" + "\n".join(shown_results)
    
    return output_text


@tool
def search_city_services_tool(query: str, category: str) -> str:
    """LangChain tool wrapper for search_city_services implementation."""
    return search_city_services(query, category)


def parse_site_impl(config_key: str = 'gu_spb_knowledge', limit: int = 10) -> str:
    """Parse site by config name and return short summary.

    If config_key is 'all', parses all configured sites in `PARSER_CONFIGS` and returns a summary.
    """
    parser = UniversalParser()
    summaries = []
    targets = []
    if config_key == 'all':
        targets = list(PARSER_CONFIGS.keys())
    else:
        # allow using both short keys and full names (case-insensitive)
        if config_key in PARSER_CONFIGS:
            targets = [config_key]
        else:
            # try to find a matching key by substring
            for k in PARSER_CONFIGS:
                if config_key.lower() in k.lower():
                    targets.append(k)
            if not targets:
                return f"–ö–æ–Ω—Ñ–∏–≥ '{config_key}' –Ω–µ –Ω–∞–π–¥–µ–Ω"

    total_items = 0
    for tk in targets[:5]:  # avoid parsing too many at once
        cfg = PARSER_CONFIGS[tk]
        try:
            items = parser.parse_site(cfg)
            count = len(items)
            total_items += count
            # summarize titles
            top_titles = [it.get('title') or it.get('name') or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è' for it in items[:limit]]
            summaries.append(f"[{tk}] parsed {count} items; top: {', '.join(top_titles[:5])}")
        except Exception as e:
            summaries.append(f"[{tk}] parse failed: {e}")



@tool
def parse_site_tool(config_key: str = 'gu_spb_knowledge', limit: int = 10) -> str:
    """LangChain tool wrapper for `parse_site_impl` - parses content from a configured site and returns a short summary."""
    return parse_site_impl(config_key, limit)

llm = GigaChat(
    credentials=GIGACHAT_CREDENTIALS, 
    verify_ssl_certs=False,
    model="GigaChat" # –Ω–µ –∑–∞–±—ã—Ç—å –ø—Ä–æ –¥–æ–±–∞–≤—Ç—å
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
)

tools = [search_city_services_tool, parse_site_tool]

# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–õ–∏—á–Ω–æ—Å—Ç—å + –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏)
system_prompt = """
–¢—ã ‚Äî ¬´–ì–æ—Ä–æ–¥—Å–∫–æ–π —Å–æ–≤–µ—Ç–Ω–∏–∫¬ª –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–∞.

–¢–í–û–Ø –°–¢–†–ê–¢–ï–ì–ò–Ø:
1. –°–Ω–∞—á–∞–ª–∞ –ø–æ—Å–º–æ—Ç—Ä–∏ –≤ –ò–°–¢–û–†–ò–Æ –î–ò–ê–õ–û–ì–ê. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç —É–∂–µ –±—ã–ª, –Ω–µ –≤—ã–∑—ã–≤–∞–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ.
2. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∏—Å–ø–æ–ª—å–∑—É–π 'search_city_services'.
4. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –∏–ª–∏ –ø–æ–ª—É—á–∏—Ç—å —Å–≤–µ–∂–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å –≤–µ–±-—Å–∞–π—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–π 'parse_site_tool' —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º config_key, –Ω–∞–ø—Ä–∏–º–µ—Ä 'gu_spb_knowledge' –∏–ª–∏ 'gu_spb_mfc'.
3. –í—Å–µ–≥–¥–∞ –æ–±—Ä–∞—â–∞–π –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ [–°–ò–°–¢–ï–ú–ù–£–Æ –ò–ù–§–û–†–ú–ê–¶–ò–Æ] –≤ –æ—Ç–≤–µ—Ç–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞. –ï—Å–ª–∏ —Ç–∞–º –Ω–∞–ø–∏—Å–∞–Ω–æ, —á—Ç–æ –∑–∞–ø–∏—Å–µ–π –º–Ω–æ–≥–æ, —Å–æ–æ–±—â–∏ –æ–± —ç—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.

–ü–†–ê–í–ò–õ–ê –ö–ê–¢–ï–ì–û–†–ò–ô (–¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞):
- 'pets': –ñ–∏–≤–æ—Ç–Ω—ã–µ (–∫–ª–∏–Ω–∏–∫–∏, –ø–ª–æ—â–∞–¥–∫–∏, —Å–æ–≤–µ—Ç—ã).
- 'documents': –ú–§–¶, –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Å–ø—Ä–∞–≤–∫–∏.
- 'health': –ó–¥–æ—Ä–æ–≤—å–µ.
- 'social': –õ—å–≥–æ—Ç—ã, –ø–æ—Å–æ–±–∏—è, –æ—Ç–∫—é—á–µ–Ω–∏—è, –ø–ª–∞–Ω–æ–≤—ã–µ —Ä–∞–±–æ—Ç—ã.
- 'iparent': –†–æ–¥–∏—Ç–µ–ª—å (–¥–µ—Ç—Å–∫–∏–µ –ø–ª–æ—â–∞–¥–∫–∏, –¥–µ—Ç—Å–∫–∏–µ —Å–∞–¥—ã, –∫—Ä—É–∂–∫–∏, —É—á—Ä–µ–∂–¥–µ–Ω–∏—è).
"""

prompt_template = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("placeholder", "{chat_history}"), # –°—é–¥–∞ –ø–æ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –ø–∞–º—è—Ç—å
    ("human", "{input}"),
    ("placeholder", "{agent_scratchpad}"), # –°—é–¥–∞ –ø–æ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –≤—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏
])

agent = create_tool_calling_agent(llm, tools, prompt_template)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

if __name__ == "__main__":
    print("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω! (–ù–∞–ø–∏—à–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞)")
    
    # –•—Ä–∞–Ω–∏–ª–∏—â–µ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞
    chat_history = [] 

    while True:
        try:
            user_input = input("\n–í—ã: ")
            if user_input.lower() in ["exit", "–≤—ã—Ö–æ–¥"]:
                break
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å
            if check_toxicity(user_input):
                print("–ë–æ—Ç: (–ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ)")
                continue
            
            # –ó–∞–ø—É—Å–∫ –∞–≥–µ–Ω—Ç–∞ —Å –∏—Å—Ç–æ—Ä–∏–µ–π
            response = agent_executor.invoke({
                "input": user_input,
                "chat_history": chat_history
            })
            
            bot_answer = response['output']
            print(f"–ë–æ—Ç: {bot_answer}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ–ø–∏—Å–∫—É –≤ –ø–∞–º—è—Ç—å
            chat_history.append(HumanMessage(content=user_input))
            chat_history.append(AIMessage(content=bot_answer))
            
            # –î–µ—Ä–∂–∏–º –≤ –ø–∞–º—è—Ç–∏ —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Ñ—Ä–∞–∑, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å —Ç–æ–∫–µ–Ω–∞–º–∏
            if len(chat_history) > 10:
                chat_history = chat_history[-10:]
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")